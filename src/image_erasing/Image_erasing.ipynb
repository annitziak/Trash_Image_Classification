{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augemntation - Image Manipulation \n",
    "1. Geometric Transformations (Rotations, Translation, Shearing, Flipping)\n",
    "2. Non- Geometric Transformations (Cropping, Noise injection, Color Space, Jitter, Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import torchvision\n",
    "from torchvision import models,datasets,transforms\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import albumentations as A #this supposedly is much faster than using torchvision\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidia\\AppData\\Local\\Temp\\ipykernel_31728\\3960484266.py:47: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=1, max_height=50, max_width=50, fill_value=0, p=0.5),  # Cutout alternative\n",
      "C:\\Users\\vidia\\AppData\\Local\\Temp\\ipykernel_31728\\3960484266.py:50: UserWarning: Argument(s) 'value' are not valid for transform Erasing\n",
      "  A.Erasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0),  # Random Erasing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def hide_and_seek(image, **kwargs):  # Accept **kwargs to prevent unexpected arguments error\n",
    "    \"\"\"\n",
    "    Applies Hide-and-Seek augmentation to the input image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray or torch.Tensor): Image tensor or array of shape (H, W, C) or (C, H, W).\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray or torch.Tensor: Augmented image.\n",
    "    \"\"\"\n",
    "    # Ensure the image is in CHW format for processing\n",
    "    if isinstance(image, np.ndarray):\n",
    "        img = np.transpose(image, (2, 0, 1))  # Convert HWC -> CHW\n",
    "        img = img.copy()  # Prevent modifying the original image\n",
    "    else:\n",
    "        img = image.clone() if isinstance(image, torch.Tensor) else image  # Handle tensors\n",
    "\n",
    "    # Get image dimensions (CHW format)\n",
    "    c, h, w = img.shape  \n",
    "\n",
    "    # Possible grid sizes (0 means no hiding)\n",
    "    grid_sizes = [0, 16, 32, 44, 56]\n",
    "    hide_prob = 0.5  # Probability of hiding a patch\n",
    "\n",
    "    # Select a random grid size\n",
    "    grid_size = random.choice(grid_sizes)\n",
    "\n",
    "    if grid_size > 0:\n",
    "        for x in range(0, w, grid_size):\n",
    "            for y in range(0, h, grid_size):\n",
    "                x_end = min(w, x + grid_size)\n",
    "                y_end = min(h, y + grid_size)\n",
    "                if random.random() <= hide_prob:\n",
    "                    img[:, y:y_end, x:x_end] = 0  # Corrected indexing\n",
    "\n",
    "    # Convert back to HWC format if input was a NumPy array\n",
    "    if isinstance(image, np.ndarray):\n",
    "        img = np.transpose(img, (1, 2, 0))  # Convert CHW -> HWC\n",
    "\n",
    "    return img\n",
    "\n",
    "augmentation_pipeline = A.Compose([\n",
    "    A.CoarseDropout(max_holes=1, max_height=50, max_width=50, fill_value=0, p=0.5),  # Cutout alternative\n",
    "    A.GridDropout(ratio=0.5, p=0.5),  # GridMask\n",
    "    A.Lambda(image=hide_and_seek, p=0.5),  # Hide-and-Seek\n",
    "    A.Erasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0),  # Random Erasing\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cardboard': 368, 'glass': 336, 'metal': 632, 'paper': 400, 'plastic': 736, 'trash': 396}\n",
      "max category is 736 of class plastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing & Expanding Classes: 100%|██████████| 6/6 [01:24<00:00, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset balanced & expanded! New images saved in ../data/dataset_erasing/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# use the pipeline to also balance the dataset\n",
    "# idea : augment images until limit set as majority_class*2\n",
    "\n",
    "\n",
    "# define the paths\n",
    "input_dir = \"../data/dataset_split/train\"  \n",
    "output_dir = \"../data/dataset_erasing/train\"  # for balanced augmented dataset\n",
    "\n",
    "# overwrite it\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)  # delete contents\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class_counts = {}\n",
    "for class_folder in os.listdir(input_dir):\n",
    "    class_path = os.path.join(input_dir, class_folder)\n",
    "    num_images = len(os.listdir(class_path))\n",
    "    class_counts[class_folder] = num_images\n",
    "\n",
    "# change this\n",
    "change_factor = 2\n",
    "\n",
    "# Find the largest class (majority class)\n",
    "max_class_size = max(class_counts.values())\n",
    "new_target_size = max_class_size * change_factor\n",
    "\n",
    "print(class_counts)\n",
    "print(f\"max category is {max_class_size} of class {max(class_counts, key=class_counts.get)}\") \n",
    "\n",
    "for class_folder, current_count in tqdm(class_counts.items(), desc=\"Balancing & Expanding Classes\"):\n",
    "    class_path = os.path.join(input_dir, class_folder)\n",
    "    augmented_class_path = os.path.join(output_dir, class_folder) #augmented images will be saved here\n",
    "    os.makedirs(augmented_class_path, exist_ok=True)\n",
    "\n",
    "    images = os.listdir(class_path)\n",
    "    \n",
    "    # decide later but this will copy all the non-augmented images as well \n",
    "    for img_name in images:\n",
    "        src_path = os.path.join(class_path, img_name)\n",
    "        dst_path = os.path.join(augmented_class_path, img_name)\n",
    "        cv2.imwrite(dst_path, cv2.imread(src_path))  # Copy image\n",
    "\n",
    "    # Compute number of extra images needed\n",
    "    num_needed = new_target_size - current_count  # balancing all classes to 2*majority\n",
    "\n",
    "    # Augment existing images\n",
    "    while num_needed > 0:\n",
    "        for img_name in images:\n",
    "            if num_needed <= 0:\n",
    "                break \n",
    "\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR (OpenCV) to RGB\n",
    "\n",
    "            augmented = augmentation_pipeline(image=image)[\"image\"]  # Albumentations output\n",
    "\n",
    "            # Save Augmented Image (Directly as PNG)\n",
    "            output_filename = f\"{os.path.splitext(img_name)[0]}_aug_{num_needed}.png\"\n",
    "            output_path = os.path.join(augmented_class_path, output_filename)\n",
    "\n",
    "            # Ensure the output is a NumPy array before saving\n",
    "            if isinstance(augmented, torch.Tensor):  \n",
    "                augmented = augmented.permute(1, 2, 0).cpu().numpy()  # Convert CHW -> HWC\n",
    "                augmented = (augmented * 255).astype(np.uint8)  # Convert from [0,1] to [0,255]\n",
    "\n",
    "            # Save the augmented image\n",
    "            cv2.imwrite(output_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
    "            num_needed -= 1\n",
    "\n",
    "print(f\"✅ Dataset balanced & expanded! New images saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardboard: 1472 images\n",
      "glass: 1472 images\n",
      "metal: 1472 images\n",
      "paper: 1472 images\n",
      "plastic: 1472 images\n",
      "trash: 1472 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to the main directory\n",
    "\n",
    "main_dir = \"../data/dataset_erasing/train\"\n",
    "# Dictionary to store image counts per subfolder\n",
    "image_counts = {}\n",
    "\n",
    "# Loop through each subdirectory\n",
    "for subfolder in os.listdir(main_dir):\n",
    "    subfolder_path = os.path.join(main_dir, subfolder)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Count images (considering common image formats)\n",
    "        image_count = len([file for file in os.listdir(subfolder_path) if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff'))])\n",
    "        \n",
    "        image_counts[subfolder] = image_count\n",
    "\n",
    "# Print the results\n",
    "for subfolder, count in image_counts.items():\n",
    "    print(f\"{subfolder}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sos remember to change the directory for the tensors\n",
    "dir = '../data/dataset_erasing/train'\n",
    "\n",
    "params = { 'batch_size':16,\n",
    "           'shuffle':True,\n",
    "           'num_workers':4 }\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.RandomResizedCrop(256), #Augmented\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "#the training dataset will be from the datase_balanced folder \n",
    "train_dataset = datasets.ImageFolder(os.path.join(dir, 'train'),transform = transform )\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(256),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "#change the dir for validationa (and test)\n",
    "dir = '../data/dataset_split'\n",
    "val_dataset = datasets.ImageFolder(os.path.join(dir, 'val'),transform = transform )\n",
    "\n",
    "\n",
    "#data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, **params)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, **params)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
