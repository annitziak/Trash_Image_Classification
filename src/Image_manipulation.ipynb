{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augemntation - Image Manipulation \n",
    "1. Geometric Transformations (Rotations, Translation, Shearing, Flipping)\n",
    "2. Non- Geometric Transformations (Cropping, Noise injection, Color Space, Jitter, Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A #this supposedly is much faster than using torchvision\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_pipeline = A.Compose([\n",
    "    # Geometric Transformations\n",
    "    A.Rotate(limit=25, p=0.5),  # rotation +-25 with prob 0.5\n",
    "    A.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.2, rotate_limit=0, p=0.3),  # translation (shift and scale but do not rotate more)\n",
    "    A.Affine(shear=5, p=0.5),  # shearing up to 5deg - does not seem that useful for us\n",
    "    A.HorizontalFlip(p=0.5),  # flipping h\n",
    "\n",
    "    # Non-Geometric Transformations\n",
    "    A.RandomResizedCrop(size=[524,524], scale=(0.7, 1.0), p=0.5),  # gets randomly 70-100% of image and resized it back to 256x256\n",
    "    A.GaussNoise(var_limit=(2, 10), p=0.2),  # less variance\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.2), # contrast for color (insetad of b&w)\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.3),  # Color Adjustments different lightning conditions\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.2),  # Kernel Blurring\n",
    "\n",
    "    A.Resize(524, 524),\n",
    "\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the pipeline to balance the dataset\n",
    "### Idea : The augmented images at each catergory must all be equal to majority_class*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cardboard': 368, 'glass': 336, 'metal': 632, 'paper': 400, 'plastic': 736, 'trash': 396}\n",
      "max category is 736 of class plastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing & Expanding Classes: 100%|██████████| 6/6 [01:48<00:00, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset balanced & expanded! New images saved in ../data/dataset_balanced/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define the paths\n",
    "input_dir = \"../data/dataset_split/train\"  \n",
    "output_dir = \"../data/dataset_balanced/train\" \n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class_counts = {}\n",
    "for class_folder in os.listdir(input_dir):\n",
    "    class_path = os.path.join(input_dir, class_folder)\n",
    "    num_images = len(os.listdir(class_path))\n",
    "    class_counts[class_folder] = num_images\n",
    "\n",
    "change_factor = 2\n",
    "\n",
    "max_class_size = max(class_counts.values())\n",
    "new_target_size = max_class_size * change_factor\n",
    "\n",
    "print(class_counts)\n",
    "print(f\"max category is {max_class_size} of class {max(class_counts, key=class_counts.get)}\") \n",
    "\n",
    "for class_folder, current_count in tqdm(class_counts.items(), desc=\"Balancing & Expanding Classes\"):\n",
    "    class_path = os.path.join(input_dir, class_folder)\n",
    "    augmented_class_path = os.path.join(output_dir, class_folder)\n",
    "    os.makedirs(augmented_class_path, exist_ok=True)\n",
    "\n",
    "    images = os.listdir(class_path)\n",
    "    \n",
    "    for img_name in images:\n",
    "        src_path = os.path.join(class_path, img_name)\n",
    "        dst_path = os.path.join(augmented_class_path, img_name)\n",
    "        cv2.imwrite(dst_path, cv2.imread(src_path))\n",
    "\n",
    "    num_needed = new_target_size - current_count\n",
    "\n",
    "    while num_needed > 0:\n",
    "        for img_name in images:\n",
    "            if num_needed <= 0:\n",
    "                break \n",
    "\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            augmented = augmentation_pipeline(image=image)[\"image\"]\n",
    "\n",
    "            output_filename = f\"{os.path.splitext(img_name)[0]}_aug_{num_needed}.png\"\n",
    "            output_path = os.path.join(augmented_class_path, output_filename)\n",
    "\n",
    "            if isinstance(augmented, torch.Tensor):  \n",
    "                augmented = augmented.permute(1, 2, 0).cpu().numpy()\n",
    "                augmented = (augmented * 255).astype(np.uint8) \n",
    "\n",
    "            cv2.imwrite(output_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
    "            num_needed -= 1\n",
    "\n",
    "print(f\"Dataset balanced & expanded! New images saved in {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
