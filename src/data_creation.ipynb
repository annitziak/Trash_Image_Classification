{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset used\n",
    "I have collected data from TrashNet, which includes images of various trash items labelled trash (these are not recyclable), then there are multiple classes of recyclable trash such as paper, cardboard, glass, metal and plastic\n",
    "\n",
    "1211 images are there for the training set and 508 images for the test set\n",
    "\n",
    "Images are pre-labelled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of training and validation set of original RealWaste dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Cardboard mapped to cardboard: 368 training, 93 validation\n",
      "Skipping Food Organics (ignored)\n",
      "Class Glass mapped to glass: 336 training, 84 validation\n",
      "Class Metal mapped to metal: 632 training, 158 validation\n",
      "Class Miscellaneous Trash mapped to trash: 396 training, 99 validation\n",
      "Class Paper mapped to paper: 400 training, 100 validation\n",
      "Class Plastic mapped to plastic: 736 training, 185 validation\n",
      "Skipping Textile Trash (ignored)\n",
      "Skipping Vegetation (ignored)\n",
      "Dataset splitting and mapping completed!\n"
     ]
    }
   ],
   "source": [
    "def create_splits(data_dir, output_dir, val_size=0.20):\n",
    "    # Define class mapping for unifying class names\n",
    "    class_mapping = {\n",
    "        \"cardboard\": \"cardboard\",  \n",
    "        \"Cardboard\": \"cardboard\",  \n",
    "        \"Glass\": \"glass\",\n",
    "        \"glass\": \"glass\",\n",
    "        \"Metal\": \"metal\",\n",
    "        \"metal\": \"metal\",\n",
    "        \"paper\": \"paper\",\n",
    "        \"Paper\": \"paper\",\n",
    "        \"plastic\": \"plastic\",\n",
    "        \"Plastic\": \"plastic\",\n",
    "        \"trash\": \"trash\",\n",
    "        \"Miscellaneous Trash\": \"trash\",\n",
    "    }\n",
    "\n",
    "    # Folders to ignore\n",
    "    ignored_folders = {\"Textile Trash\", \"Vegetation\",\"Food Organics\"}\n",
    "\n",
    "    train_dir = os.path.join(output_dir, 'train')\n",
    "    val_dir = os.path.join(output_dir, 'val')\n",
    "\n",
    "    for d in [train_dir, val_dir]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        if class_name in ignored_folders:\n",
    "            print(f\"Skipping {class_name} (ignored)\")\n",
    "            continue\n",
    "\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        \n",
    "        mapped_class = class_mapping.get(class_name, class_name)\n",
    "\n",
    "        for d in [train_dir, val_dir]:\n",
    "            class_dir_out = os.path.join(d, mapped_class)\n",
    "            os.makedirs(class_dir_out, exist_ok=True)\n",
    "\n",
    "        images = [os.path.join(class_dir, img) for img in os.listdir(class_dir) if img.endswith(('png', 'jpg', 'jpeg'))]\n",
    "        train_images, val_images = train_test_split(images, test_size=val_size, random_state=42)\n",
    "        \n",
    "        print(f\"Class {class_name} mapped to {mapped_class}: {len(train_images)} training, {len(val_images)} validation\")\n",
    "\n",
    "        def copy_images(image_list, output_dir, split_type):\n",
    "            for image in image_list:\n",
    "                dest = os.path.join(output_dir, mapped_class, os.path.basename(image))\n",
    "                shutil.copy(image, dest)\n",
    "                records.append((os.path.basename(image), class_name, mapped_class, split_type))\n",
    "\n",
    "        copy_images(train_images, train_dir, \"train\")\n",
    "        copy_images(val_images, val_dir, \"val\")\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['filename', 'original_class', 'mapped_class', 'split_type'])\n",
    "    df.to_csv(os.path.join(output_dir, 'class_mapping.csv'), index=False)\n",
    "\n",
    "    print(\"Dataset splitting and mapping completed!\")\n",
    "\n",
    "original_data_dir = '../data/realwaste-main/RealWaste'\n",
    "output_data_dir = '../data/dataset_split'\n",
    "\n",
    "# Create splits\n",
    "create_splits(original_data_dir, output_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the test set from the TrashNet and add to data_split folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying 403 images from cardboard to cardboard in test set...\n",
      "Copying 501 images from glass to glass in test set...\n",
      "Copying 410 images from metal to metal in test set...\n",
      "Copying 594 images from paper to paper in test set...\n",
      "Copying 482 images from plastic to plastic in test set...\n",
      "Copying 137 images from trash to trash in test set...\n",
      "All images successfully copied to the test set!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def copy_all_to_test(data_dir, output_dir):\n",
    "    # Define class mapping for unifying class names\n",
    "    class_mapping = {\n",
    "        \"cardboard\": \"cardboard\",  \n",
    "        \"Cardboard\": \"cardboard\",  \n",
    "        \"Glass\": \"glass\",\n",
    "        \"glass\": \"glass\",\n",
    "        \"Metal\": \"metal\",\n",
    "        \"metal\": \"metal\",\n",
    "        \"paper\": \"paper\",\n",
    "        \"Paper\": \"paper\",\n",
    "        \"plastic\": \"plastic\",\n",
    "        \"Plastic\": \"plastic\",\n",
    "        \"trash\": \"trash\",\n",
    "        \"Miscellaneous Trash\": \"trash\",\n",
    "    }\n",
    "\n",
    "    # Folders to ignore\n",
    "    ignored_folders = {\"Textile Trash\", \"Vegetation\",\"Food Organics\"}\n",
    "\n",
    "    test_dir = os.path.join(output_dir, 'test')\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        if class_name in ignored_folders:\n",
    "            print(f\"Skipping {class_name} (ignored)\")\n",
    "            continue\n",
    "\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        mapped_class = class_mapping.get(class_name, class_name)\n",
    "\n",
    "        class_test_dir = os.path.join(test_dir, mapped_class)\n",
    "        os.makedirs(class_test_dir, exist_ok=True)\n",
    "\n",
    "        images = [os.path.join(class_dir, img) for img in os.listdir(class_dir) if img.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "        print(f\"Copying {len(images)} images from {class_name} to {mapped_class} in test set...\")\n",
    "\n",
    "        for image in images:\n",
    "            dest = os.path.join(class_test_dir, os.path.basename(image))\n",
    "            shutil.copy(image, dest)\n",
    "            records.append((os.path.basename(image), class_name, mapped_class, \"test\"))\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['filename', 'original_class', 'mapped_class', 'split_type'])\n",
    "    df.to_csv(os.path.join(output_dir, 'class_mapping.csv'), index=False)\n",
    "\n",
    "    print(\"All images successfully copied to the test set!\")\n",
    "\n",
    "original_data_dir = '../data/dataset-resized'\n",
    "output_data_dir = '../data/dataset_split'\n",
    "\n",
    "copy_all_to_test(original_data_dir, output_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of the images that each folder has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardboard: 1472 images\n",
      "glass: 1472 images\n",
      "metal: 1472 images\n",
      "paper: 1472 images\n",
      "plastic: 1472 images\n",
      "trash: 1472 images\n"
     ]
    }
   ],
   "source": [
    "main_dir = \"../data/combined_dataset\"\n",
    "\n",
    "image_counts = {}\n",
    "\n",
    "for subfolder in os.listdir(main_dir):\n",
    "    subfolder_path = os.path.join(main_dir, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path):\n",
    "        image_count = len([file for file in os.listdir(subfolder_path) if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff'))])\n",
    "        image_counts[subfolder] = image_count\n",
    "\n",
    "for subfolder, count in image_counts.items():\n",
    "    print(f\"{subfolder}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates the combine dataset by sampling the results of the different augmented techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardboard: Copied 1472 images (368 originals, 1104 augmented - diffusion: 491 (orig: 123, aug: 368, 33.4%), manipulation: 491 (orig: 123, aug: 368, 33.4%), erasing: 490 (orig: 122, aug: 368, 33.3%))\n",
      "glass: Copied 1472 images (336 originals, 1136 augmented - diffusion: 491 (orig: 112, aug: 379, 33.4%), manipulation: 491 (orig: 112, aug: 379, 33.4%), erasing: 490 (orig: 112, aug: 378, 33.3%))\n",
      "plastic: Copied 1472 images (736 originals, 736 augmented - diffusion: 492 (orig: 246, aug: 246, 33.4%), manipulation: 490 (orig: 245, aug: 245, 33.3%), erasing: 490 (orig: 245, aug: 245, 33.3%))\n",
      "trash: Copied 1472 images (396 originals, 1076 augmented - diffusion: 491 (orig: 132, aug: 359, 33.4%), manipulation: 491 (orig: 132, aug: 359, 33.4%), erasing: 490 (orig: 132, aug: 358, 33.3%))\n",
      "paper: Copied 1472 images (400 originals, 1072 augmented - diffusion: 492 (orig: 134, aug: 358, 33.4%), manipulation: 490 (orig: 133, aug: 357, 33.3%), erasing: 490 (orig: 133, aug: 357, 33.3%))\n",
      "metal: Copied 1472 images (632 originals, 840 augmented - diffusion: 491 (orig: 211, aug: 280, 33.4%), manipulation: 491 (orig: 211, aug: 280, 33.4%), erasing: 490 (orig: 210, aug: 280, 33.3%))\n",
      "Combined dataset creation completed!\n"
     ]
    }
   ],
   "source": [
    "source_dirs = {\n",
    "    'diffusion': '../data/dataset_diffusion_balanced/train',\n",
    "    'manipulation': '../data/dataset_balanced/train',\n",
    "    'erasing': '../data/dataset_erasing_augmented/train'\n",
    "}\n",
    "output_dir = '../data/combined_dataset'\n",
    "categories = ['cardboard', 'glass', 'plastic', 'trash', 'paper', 'metal']\n",
    "\n",
    "aug_suffixes = {\n",
    "    'diffusion': 'aug_diffusion_',\n",
    "    'manipulation': 'aug_manipulation_',\n",
    "    'erasing': 'aug_erasing_'\n",
    "}\n",
    "\n",
    "# Target number of images per category\n",
    "TOTAL_IMAGES = 1472\n",
    "\n",
    "def create_output_structure():\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(output_dir, category)\n",
    "        if not os.path.exists(category_path):\n",
    "            os.makedirs(category_path)\n",
    "\n",
    "def get_original_images(source_dir, category):\n",
    "    category_path = os.path.join(source_dir, category)\n",
    "    return [f for f in os.listdir(category_path) \n",
    "            if os.path.isfile(os.path.join(category_path, f)) \n",
    "            and f.lower().endswith(('.jpg', '.jpeg', '.png')) \n",
    "            and 'aug' not in f.lower()]\n",
    "\n",
    "def get_augmented_images_for_instance(source_dir, category, instance_base):\n",
    "    category_path = os.path.join(source_dir, category)\n",
    "    return [f for f in os.listdir(category_path) \n",
    "            if os.path.isfile(os.path.join(category_path, f)) \n",
    "            and f.lower().endswith(('.jpg', '.jpeg', '.png')) \n",
    "            and 'aug' in f.lower() \n",
    "            and f.startswith(instance_base)]\n",
    "\n",
    "def get_all_augmented_images(source_dir, category):\n",
    "    category_path = os.path.join(source_dir, category)\n",
    "    return [f for f in os.listdir(category_path) \n",
    "            if os.path.isfile(os.path.join(category_path, f)) \n",
    "            and f.lower().endswith(('.jpg', '.jpeg', '.png')) \n",
    "            and 'aug' in f.lower()]\n",
    "\n",
    "def validate_input_datasets():\n",
    "    for source, dir_path in source_dirs.items():\n",
    "        for category in categories:\n",
    "            total_images = len([f for f in os.listdir(os.path.join(dir_path, category)) \n",
    "                              if os.path.isfile(os.path.join(dir_path, category, f)) \n",
    "                              and f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            if total_images != TOTAL_IMAGES:\n",
    "                raise ValueError(f\"Expected {TOTAL_IMAGES} total images in {source}/{category}, \"\n",
    "                               f\"but found {total_images}.\")\n",
    "\n",
    "def create_combined_dataset():\n",
    "    validate_input_datasets()\n",
    "    create_output_structure()\n",
    "    \n",
    "    for category in categories:\n",
    "        orig_by_source = {source: get_original_images(dir_path, category) \n",
    "                         for source, dir_path in source_dirs.items()}\n",
    "        \n",
    "        orig_images_with_sources = {}\n",
    "        for source, images in orig_by_source.items():\n",
    "            for img in images:\n",
    "                if img not in orig_images_with_sources:\n",
    "                    orig_images_with_sources[img] = []\n",
    "                orig_images_with_sources[img].append(source)\n",
    "        \n",
    "        orig_images_list = list(orig_images_with_sources.keys())\n",
    "        total_orig_images = len(orig_images_list)\n",
    "        \n",
    "        total_aug_images = TOTAL_IMAGES - total_orig_images\n",
    "        aug_per_source = total_aug_images // 3\n",
    "        aug_remainder = total_aug_images % 3\n",
    "        \n",
    "        aug_target_counts = {}\n",
    "        remaining = aug_remainder\n",
    "        for source in source_dirs:\n",
    "            aug_target_counts[source] = aug_per_source + (1 if remaining > 0 else 0)\n",
    "            remaining -= 1 if remaining > 0 else 0\n",
    "        \n",
    "        aug_by_source = {source: get_all_augmented_images(dir_path, category) \n",
    "                        for source, dir_path in source_dirs.items()}\n",
    "        used_aug_images = {source: set() for source in source_dirs}\n",
    "        \n",
    "        count_by_source = {'diffusion': 0, 'manipulation': 0, 'erasing': 0}\n",
    "        orig_copied_by_source = {'diffusion': 0, 'manipulation': 0, 'erasing': 0}\n",
    "        aug_images_copied = {'diffusion': 0, 'manipulation': 0, 'erasing': 0}\n",
    "        \n",
    "        # Step 1: Copy all originals, balancing sources\n",
    "        orig_images_shuffled = orig_images_list.copy()\n",
    "        random.shuffle(orig_images_shuffled)\n",
    "        \n",
    "        orig_per_source = total_orig_images // 3\n",
    "        orig_remainder = total_orig_images % 3\n",
    "        orig_counts = {}\n",
    "        remaining_orig = orig_remainder\n",
    "        for source in source_dirs:\n",
    "            orig_counts[source] = orig_per_source + (1 if remaining_orig > 0 else 0)\n",
    "            remaining_orig -= 1 if remaining_orig > 0 else 0\n",
    "        \n",
    "        image_counter = 1\n",
    "        for orig_image in orig_images_shuffled:\n",
    "            if image_counter > TOTAL_IMAGES:\n",
    "                break\n",
    "            sources = orig_images_with_sources[orig_image]\n",
    "            selected_source = None\n",
    "            for source in sources:\n",
    "                if orig_copied_by_source[source] < orig_counts[source]:\n",
    "                    selected_source = source\n",
    "                    break\n",
    "            if selected_source is None:\n",
    "                continue\n",
    "            \n",
    "            suffix = 'original_'\n",
    "            src_path = os.path.join(source_dirs[selected_source], category, orig_image)\n",
    "            ext = os.path.splitext(orig_image)[1]\n",
    "            new_filename = f\"{category}_{suffix}{image_counter}{ext}\"\n",
    "            dst_path = os.path.join(output_dir, category, new_filename)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            count_by_source[selected_source] += 1\n",
    "            orig_copied_by_source[selected_source] += 1\n",
    "            image_counter += 1\n",
    "        \n",
    "        orig_copied = sum(orig_copied_by_source.values())\n",
    "        \n",
    "        # Step 2: Copy instance-specific augmented images to meet aug_target_counts\n",
    "        for orig_image in orig_images_shuffled:\n",
    "            if image_counter > TOTAL_IMAGES:\n",
    "                break\n",
    "            \n",
    "            instance_base = os.path.splitext(orig_image)[0]\n",
    "            aug_available = {}\n",
    "            for source, dir_path in source_dirs.items():\n",
    "                aug_images = get_augmented_images_for_instance(dir_path, category, instance_base)\n",
    "                aug_available[source] = [img for img in aug_images if img not in used_aug_images[source]]\n",
    "            \n",
    "            sources_sorted = sorted(source_dirs.keys(), \n",
    "                                  key=lambda s: (aug_target_counts[s] - aug_images_copied[s]), \n",
    "                                  reverse=True)\n",
    "            for source in sources_sorted:\n",
    "                if image_counter > TOTAL_IMAGES:\n",
    "                    break\n",
    "                if aug_images_copied[source] >= aug_target_counts[source]:\n",
    "                    continue\n",
    "                if aug_available[source]:\n",
    "                    aug_image = random.choice(aug_available[source])\n",
    "                    suffix = aug_suffixes[source]\n",
    "                    src_path = os.path.join(source_dirs[source], category, aug_image)\n",
    "                    ext = os.path.splitext(aug_image)[1]\n",
    "                    new_filename = f\"{category}_{suffix}{image_counter}{ext}\"\n",
    "                    dst_path = os.path.join(output_dir, category, new_filename)\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "                    used_aug_images[source].add(aug_image)\n",
    "                    if aug_image in aug_by_source[source]:\n",
    "                        aug_by_source[source].remove(aug_image)\n",
    "                    count_by_source[source] += 1\n",
    "                    aug_images_copied[source] += 1\n",
    "                    image_counter += 1\n",
    "        \n",
    "        # Step 3: Fill remaining slots to reach aug_target_counts exactly\n",
    "        for source in source_dirs:\n",
    "            remaining_for_source = max(0, aug_target_counts[source] - aug_images_copied[source])\n",
    "            if remaining_for_source > 0 and image_counter <= TOTAL_IMAGES:\n",
    "                available_images = [img for img in aug_by_source[source] if img not in used_aug_images[source]]\n",
    "                if len(available_images) < remaining_for_source:\n",
    "                    raise ValueError(f\"Not enough remaining augmented images in {source}/{category}: \"\n",
    "                                   f\"found {len(available_images)}, need {remaining_for_source}\")\n",
    "                selected_images = random.sample(available_images, remaining_for_source)\n",
    "                suffix = aug_suffixes[source]\n",
    "                for image in selected_images:\n",
    "                    if image_counter > TOTAL_IMAGES:\n",
    "                        break\n",
    "                    src_path = os.path.join(source_dirs[source], category, image)\n",
    "                    ext = os.path.splitext(image)[1]\n",
    "                    new_filename = f\"{category}_{suffix}{image_counter}{ext}\"\n",
    "                    dst_path = os.path.join(output_dir, category, new_filename)\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "                    count_by_source[source] += 1\n",
    "                    aug_images_copied[source] += 1\n",
    "                    image_counter += 1\n",
    "        \n",
    "        total_copied = sum(count_by_source.values())\n",
    "        total_copied = min(total_copied, TOTAL_IMAGES)\n",
    "        total_aug_copied = sum(aug_images_copied.values())\n",
    "        \n",
    "        print(f\"{category}: Copied {total_copied} images \"\n",
    "              f\"({orig_copied} originals, {total_aug_copied} augmented - \"\n",
    "              f\"diffusion: {count_by_source['diffusion']} (orig: {orig_copied_by_source['diffusion']}, aug: {aug_images_copied['diffusion']}, {count_by_source['diffusion']/total_copied:.1%}), \"\n",
    "              f\"manipulation: {count_by_source['manipulation']} (orig: {orig_copied_by_source['manipulation']}, aug: {aug_images_copied['manipulation']}, {count_by_source['manipulation']/total_copied:.1%}), \"\n",
    "              f\"erasing: {count_by_source['erasing']} (orig: {orig_copied_by_source['erasing']}, aug: {aug_images_copied['erasing']}, {count_by_source['erasing']/total_copied:.1%}))\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)\n",
    "    try:\n",
    "        create_combined_dataset()\n",
    "        print(\"Combined dataset creation completed!\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
