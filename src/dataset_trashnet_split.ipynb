{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset used\n",
    "I have collected data from TrashNet, which includes images of various trash items labelled trash (these are not recyclable), then there are multiple classes of recyclable trash such as paper, cardboard, glass, metal and plastic\n",
    "\n",
    "1211 images are there for the training set and 508 images for the test set\n",
    "\n",
    "Images are pre-labelled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_splits(data_dir, output_dir, test_size=0.10, val_size=0.15):\n",
    "    # Create directories for the train, validation, and test sets\n",
    "    train_dir = os.path.join(output_dir, 'train')\n",
    "    val_dir = os.path.join(output_dir, 'val')\n",
    "    test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "    for d in [train_dir, val_dir, test_dir]:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "\n",
    "    # Process each class directory\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        \n",
    "        # Create class directories in train, val, test\n",
    "        for d in [train_dir, val_dir, test_dir]:\n",
    "            class_dir_out = os.path.join(d, class_name)\n",
    "            if not os.path.exists(class_dir_out):\n",
    "                os.makedirs(class_dir_out)\n",
    "        \n",
    "        # Get all images and split them\n",
    "        images = [os.path.join(class_dir, img) for img in os.listdir(class_dir) if img.endswith(('png', 'jpg', 'jpeg'))]\n",
    "        train_val, test_images = train_test_split(images, test_size=test_size, random_state=42)\n",
    "        train_images, val_images = train_test_split(train_val, test_size=val_size / (1 - test_size), random_state=42)\n",
    "        print(f\"Class {class_name}: {len(train_images)} training, {len(val_images)} validation\")\n",
    "        \n",
    "        # Function to copy images to the respective directories\n",
    "        def copy_images(image_list, output_dir):\n",
    "            for image in image_list:\n",
    "                dest = os.path.join(output_dir, class_name, os.path.basename(image))\n",
    "                shutil.copy(image, dest)\n",
    "\n",
    "        # Copy images to their respective directories\n",
    "        copy_images(train_images, train_dir)\n",
    "        copy_images(val_images, val_dir)\n",
    "        copy_images(test_images, test_dir)\n",
    "\n",
    "# Set the paths\n",
    "original_data_dir = r'C:\\Users\\vidia\\OneDrive\\Documents\\mlp_proj\\mlp_assignment\\data\\dataset-resized'\n",
    "output_data_dir = r'C:\\Users\\vidia\\OneDrive\\Documents\\mlp_proj\\mlp_assignment\\src\\dataset_split'\n",
    "\n",
    "# Create splits\n",
    "create_splits(original_data_dir, output_data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject_kernel",
   "language": "python",
   "name": "myproject_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
