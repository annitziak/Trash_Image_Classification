{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models,datasets,transforms\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../data/dataset_split'\n",
    "\n",
    "params = { 'batch_size':16,\n",
    "           'shuffle':True,\n",
    "           'num_workers':4 }\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.RandomResizedCrop(256), #Augmented\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "train_dataset = datasets.ImageFolder(os.path.join(dir, 'train'),transform = transform )\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(256),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "val_dataset = datasets.ImageFolder(os.path.join(dir, 'val'),transform = transform )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, **params)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, **params)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset = 3196\n",
      " Val dataset = 802\n",
      "Classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset = {}\\n'.format(len(train_dataset)),'Val dataset = {}'.format(len(val_dataset)))\n",
    "print('Classes = {}'.format(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, num_epochs=25):\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}'.format(epoch+1))\n",
    "        \n",
    "        # Train dataset\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        size = len(train_dataset)\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "            \n",
    "        train_loss = train_loss / size\n",
    "        train_acc = train_correct.double() / size\n",
    "            \n",
    "        print('Training Loss: {} Acc: {}'.format(train_loss, train_acc))\n",
    "        \n",
    "        # Val dataset\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        size = len(val_dataset)\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += torch.sum(preds == labels.data) \n",
    "\n",
    "        val_loss = val_loss / size\n",
    "        val_acc = val_correct.double() / size    \n",
    "\n",
    "        print('Validation Loss: {} Acc: {}'.format(val_loss, val_acc))    \n",
    "            \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "\n",
    "    print('Best val Acc: {}'.format(best_acc))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vidia\\anaconda3\\envs\\mlp\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vidia\\anaconda3\\envs\\mlp\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "net = torchvision.models.densenet121(pretrained=True)\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False # freeze all the weights\n",
    "\n",
    "ft = net.classifier.in_features # final layer of the densenet\n",
    "net.classifier = nn.Linear(ft, 6) # new layer according to our dataset with weights unfrozen\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# only final layer optimized\n",
    "optimizer = optim.SGD(net.classifier.parameters(), lr=0.0001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 1.4450444723100626 Acc: 0.4712140175219024\n",
      "Validation Loss: 1.2966548840601249 Acc: 0.5423940149625935\n",
      "Epoch 2\n",
      "Training Loss: 1.2827505905726675 Acc: 0.564142678347935\n",
      "Validation Loss: 1.1484152637514984 Acc: 0.6334164588528678\n",
      "Epoch 3\n",
      "Training Loss: 1.1948394912652887 Acc: 0.6032540675844806\n",
      "Validation Loss: 1.0473769522664553 Acc: 0.6770573566084788\n",
      "Epoch 4\n",
      "Training Loss: 1.1166663980006575 Acc: 0.636107634543179\n",
      "Validation Loss: 0.9741521446484878 Acc: 0.6895261845386533\n",
      "Epoch 5\n",
      "Training Loss: 1.0670624645391902 Acc: 0.6561326658322904\n",
      "Validation Loss: 0.9403985952795889 Acc: 0.6932668329177057\n",
      "Epoch 6\n",
      "Training Loss: 1.018818481395182 Acc: 0.6799123904881101\n",
      "Validation Loss: 0.8823913769829006 Acc: 0.7331670822942643\n",
      "Epoch 7\n",
      "Training Loss: 0.9908433306649869 Acc: 0.6695869837296621\n",
      "Validation Loss: 0.8524195687134665 Acc: 0.7506234413965087\n",
      "Epoch 8\n",
      "Training Loss: 0.9368590062043545 Acc: 0.7043178973717147\n",
      "Validation Loss: 0.8337934412563828 Acc: 0.7406483790523691\n",
      "Epoch 9\n",
      "Training Loss: 0.9252377714919805 Acc: 0.7115143929912391\n",
      "Validation Loss: 0.8052084813094199 Acc: 0.7581047381546134\n",
      "Epoch 10\n",
      "Training Loss: 0.9043845505231015 Acc: 0.7130788485607009\n",
      "Validation Loss: 0.7848537635624855 Acc: 0.7593516209476309\n",
      "Epoch 11\n",
      "Training Loss: 0.884226801100004 Acc: 0.7118272841051314\n",
      "Validation Loss: 0.7582132092437839 Acc: 0.7680798004987531\n",
      "Epoch 12\n",
      "Training Loss: 0.8681399984115056 Acc: 0.7149561952440551\n",
      "Validation Loss: 0.7426237167414287 Acc: 0.770573566084788\n",
      "Epoch 13\n",
      "Training Loss: 0.8575610671383568 Acc: 0.7155819774718398\n",
      "Validation Loss: 0.7329452393060908 Acc: 0.773067331670823\n",
      "Epoch 14\n",
      "Training Loss: 0.8416092248822333 Acc: 0.7230913642052565\n",
      "Validation Loss: 0.7206866643078013 Acc: 0.7817955112219451\n",
      "Epoch 15\n",
      "Training Loss: 0.8302617834715431 Acc: 0.7340425531914894\n",
      "Validation Loss: 0.7181475507350931 Acc: 0.7680798004987531\n",
      "Epoch 16\n",
      "Training Loss: 0.8105377657094198 Acc: 0.7284105131414268\n",
      "Validation Loss: 0.7071870186010798 Acc: 0.7693266832917706\n",
      "Epoch 17\n",
      "Training Loss: 0.815456376281638 Acc: 0.7284105131414268\n",
      "Validation Loss: 0.6948640408955905 Acc: 0.7817955112219451\n",
      "Epoch 18\n",
      "Training Loss: 0.7998294624428874 Acc: 0.723404255319149\n",
      "Validation Loss: 0.6910367521859166 Acc: 0.7743142144638404\n",
      "Epoch 19\n",
      "Training Loss: 0.772889364198391 Acc: 0.7478097622027534\n",
      "Validation Loss: 0.6775425610102322 Acc: 0.7817955112219451\n",
      "Epoch 20\n",
      "Training Loss: 0.7712361855113014 Acc: 0.7446808510638298\n",
      "Validation Loss: 0.6606387542072971 Acc: 0.7855361596009975\n",
      "Epoch 21\n",
      "Training Loss: 0.7550160050093755 Acc: 0.7496871088861077\n",
      "Validation Loss: 0.6600269094072375 Acc: 0.7855361596009975\n",
      "Epoch 22\n",
      "Training Loss: 0.7598562856042788 Acc: 0.7440550688360451\n",
      "Validation Loss: 0.6550994659599818 Acc: 0.7830423940149626\n",
      "Epoch 23\n",
      "Training Loss: 0.7431929234569153 Acc: 0.7596996245306633\n",
      "Validation Loss: 0.6424395984545016 Acc: 0.7892768079800498\n",
      "Epoch 24\n",
      "Training Loss: 0.749379795991136 Acc: 0.7415519399249061\n",
      "Validation Loss: 0.6433047997089395 Acc: 0.7992518703241895\n",
      "Epoch 25\n",
      "Training Loss: 0.7472418428064138 Acc: 0.7471839799749687\n",
      "Validation Loss: 0.6363342836907975 Acc: 0.7830423940149626\n",
      "Epoch 26\n",
      "Training Loss: 0.7389886439667178 Acc: 0.7537546933667084\n",
      "Validation Loss: 0.6327598347628206 Acc: 0.7930174563591023\n",
      "Epoch 27\n",
      "Training Loss: 0.7344024891548968 Acc: 0.7587609511889862\n",
      "Validation Loss: 0.6368582182542939 Acc: 0.7793017456359103\n",
      "Epoch 28\n",
      "Training Loss: 0.7273105903620714 Acc: 0.7581351689612015\n",
      "Validation Loss: 0.6199771170306979 Acc: 0.7967581047381546\n",
      "Epoch 29\n",
      "Training Loss: 0.7113195999393773 Acc: 0.7700250312891114\n",
      "Validation Loss: 0.6320423746941393 Acc: 0.7830423940149626\n",
      "Epoch 30\n",
      "Training Loss: 0.7100888175868869 Acc: 0.7634543178973717\n",
      "Validation Loss: 0.6092689674988649 Acc: 0.7980049875311721\n",
      "Epoch 31\n",
      "Training Loss: 0.7141662631673419 Acc: 0.7543804755944932\n",
      "Validation Loss: 0.6227848917766104 Acc: 0.7830423940149626\n",
      "Epoch 32\n",
      "Training Loss: 0.6915512152846077 Acc: 0.7712765957446809\n",
      "Validation Loss: 0.6062113587695761 Acc: 0.7892768079800498\n",
      "Epoch 33\n",
      "Training Loss: 0.6925678371040335 Acc: 0.7668961201501877\n",
      "Validation Loss: 0.5977489868378699 Acc: 0.8067331670822943\n",
      "Epoch 34\n",
      "Training Loss: 0.7013997746647822 Acc: 0.7578222778473092\n",
      "Validation Loss: 0.5985388178331894 Acc: 0.8029925187032418\n",
      "Epoch 35\n",
      "Training Loss: 0.6921792648014646 Acc: 0.7653316645807259\n",
      "Validation Loss: 0.596581793411117 Acc: 0.8017456359102244\n",
      "Epoch 36\n",
      "Training Loss: 0.6930867787893484 Acc: 0.759386733416771\n",
      "Validation Loss: 0.6160418926331765 Acc: 0.7955112219451371\n",
      "Epoch 37\n",
      "Training Loss: 0.6936006544528527 Acc: 0.7581351689612015\n",
      "Validation Loss: 0.5941259486419601 Acc: 0.7992518703241895\n",
      "Epoch 38\n",
      "Training Loss: 0.6671222133540988 Acc: 0.769712140175219\n",
      "Validation Loss: 0.5931390684441735 Acc: 0.7980049875311721\n",
      "Epoch 39\n",
      "Training Loss: 0.68382481781204 Acc: 0.7665832290362954\n",
      "Validation Loss: 0.5976481758002331 Acc: 0.7942643391521197\n",
      "Epoch 40\n",
      "Training Loss: 0.665816272976103 Acc: 0.7769086357947435\n",
      "Validation Loss: 0.5929222155241598 Acc: 0.8029925187032418\n",
      "Epoch 41\n",
      "Training Loss: 0.6682143619421576 Acc: 0.7740926157697121\n",
      "Validation Loss: 0.5857066121927818 Acc: 0.800498753117207\n",
      "Epoch 42\n",
      "Training Loss: 0.6658973716973363 Acc: 0.7772215269086358\n",
      "Validation Loss: 0.5823200686168195 Acc: 0.7980049875311721\n",
      "Epoch 43\n",
      "Training Loss: 0.6746194551748985 Acc: 0.769712140175219\n",
      "Validation Loss: 0.5838240808679576 Acc: 0.8029925187032418\n",
      "Epoch 44\n",
      "Training Loss: 0.6611351963127957 Acc: 0.769712140175219\n",
      "Validation Loss: 0.5694932040579599 Acc: 0.8054862842892768\n",
      "Epoch 45\n",
      "Training Loss: 0.662362156358917 Acc: 0.7725281602002503\n",
      "Validation Loss: 0.5745049637749308 Acc: 0.8042394014962594\n",
      "Epoch 46\n",
      "Training Loss: 0.6630532493131779 Acc: 0.7693992490613266\n",
      "Validation Loss: 0.5840712764911223 Acc: 0.7930174563591023\n",
      "Epoch 47\n",
      "Training Loss: 0.6583964700842083 Acc: 0.7740926157697121\n",
      "Validation Loss: 0.5678757214152308 Acc: 0.8092269326683291\n",
      "Epoch 48\n",
      "Training Loss: 0.6656159546706495 Acc: 0.766270337922403\n",
      "Validation Loss: 0.5846200132756459 Acc: 0.7980049875311721\n",
      "Epoch 49\n",
      "Training Loss: 0.6427389294543164 Acc: 0.7837922403003754\n",
      "Validation Loss: 0.5690357224305075 Acc: 0.7967581047381546\n",
      "Epoch 50\n",
      "Training Loss: 0.6503633771730454 Acc: 0.7800375469336671\n",
      "Validation Loss: 0.5817321562707573 Acc: 0.8017456359102244\n",
      "Epoch 51\n",
      "Training Loss: 0.6494063575366262 Acc: 0.7759699624530664\n",
      "Validation Loss: 0.5674633587387732 Acc: 0.8067331670822943\n",
      "Epoch 52\n",
      "Training Loss: 0.6529512019718394 Acc: 0.7712765957446809\n",
      "Validation Loss: 0.5700620846855373 Acc: 0.8117206982543641\n",
      "Epoch 53\n",
      "Training Loss: 0.6455604489216668 Acc: 0.7728410513141427\n",
      "Validation Loss: 0.582086730701965 Acc: 0.8017456359102244\n",
      "Epoch 54\n",
      "Training Loss: 0.6346582850914574 Acc: 0.785982478097622\n",
      "Validation Loss: 0.5794198389706245 Acc: 0.7930174563591023\n",
      "Epoch 55\n",
      "Training Loss: 0.6465154649020733 Acc: 0.7775344180225282\n",
      "Validation Loss: 0.5700594777255285 Acc: 0.8042394014962594\n",
      "Epoch 56\n",
      "Training Loss: 0.6342325003185917 Acc: 0.7866082603254068\n",
      "Validation Loss: 0.5665794948389999 Acc: 0.8079800498753117\n",
      "Epoch 57\n",
      "Training Loss: 0.6278445906424254 Acc: 0.7800375469336671\n",
      "Validation Loss: 0.5606372695314022 Acc: 0.8029925187032418\n",
      "Epoch 58\n",
      "Training Loss: 0.6285391407406823 Acc: 0.7831664580725908\n",
      "Validation Loss: 0.5610511762245635 Acc: 0.8017456359102244\n",
      "Epoch 59\n",
      "Training Loss: 0.6336987677294859 Acc: 0.7828535669586983\n",
      "Validation Loss: 0.5646163042346736 Acc: 0.7992518703241895\n",
      "Epoch 60\n",
      "Training Loss: 0.633518710974907 Acc: 0.7806633291614519\n",
      "Validation Loss: 0.5688505868364748 Acc: 0.8017456359102244\n",
      "Epoch 61\n",
      "Training Loss: 0.6237461845925514 Acc: 0.7800375469336671\n",
      "Validation Loss: 0.5503655461004547 Acc: 0.8117206982543641\n",
      "Epoch 62\n",
      "Training Loss: 0.6260936667161232 Acc: 0.7837922403003754\n",
      "Validation Loss: 0.5613134573225369 Acc: 0.814214463840399\n",
      "Epoch 63\n",
      "Training Loss: 0.6215837274833078 Acc: 0.7875469336670838\n",
      "Validation Loss: 0.5425561616322644 Acc: 0.816708229426434\n",
      "Epoch 64\n",
      "Training Loss: 0.6207000513995843 Acc: 0.7928660826032541\n",
      "Validation Loss: 0.5562895795056351 Acc: 0.8117206982543641\n",
      "Epoch 65\n",
      "Training Loss: 0.6247863315596002 Acc: 0.7878598247809763\n",
      "Validation Loss: 0.550182605324838 Acc: 0.814214463840399\n",
      "Epoch 66\n",
      "Training Loss: 0.620266402058667 Acc: 0.7853566958698373\n",
      "Validation Loss: 0.5445151902419373 Acc: 0.8192019950124688\n",
      "Epoch 67\n",
      "Training Loss: 0.6094654473255812 Acc: 0.7950563204005007\n",
      "Validation Loss: 0.5441705787048078 Acc: 0.8192019950124688\n",
      "Epoch 68\n",
      "Training Loss: 0.5970907943120438 Acc: 0.7956821026282853\n",
      "Validation Loss: 0.5456694638045352 Acc: 0.8104738154613467\n",
      "Epoch 69\n",
      "Training Loss: 0.6179660587794192 Acc: 0.7903629536921152\n",
      "Validation Loss: 0.5429188390622413 Acc: 0.8204488778054863\n",
      "Epoch 70\n",
      "Training Loss: 0.629814986964788 Acc: 0.7822277847309136\n",
      "Validation Loss: 0.5439737529825985 Acc: 0.8117206982543641\n",
      "Epoch 71\n",
      "Training Loss: 0.6186113688763749 Acc: 0.782540675844806\n",
      "Validation Loss: 0.5415591399865852 Acc: 0.814214463840399\n",
      "Epoch 72\n",
      "Training Loss: 0.6044194495424311 Acc: 0.7931789737171464\n",
      "Validation Loss: 0.5517341335515429 Acc: 0.8129675810473815\n",
      "Epoch 73\n",
      "Training Loss: 0.5982007781913194 Acc: 0.799749687108886\n",
      "Validation Loss: 0.537916974385183 Acc: 0.814214463840399\n",
      "Epoch 74\n",
      "Training Loss: 0.6201603198528887 Acc: 0.7809762202753442\n",
      "Validation Loss: 0.5550215065739399 Acc: 0.8117206982543641\n",
      "Epoch 75\n",
      "Training Loss: 0.6113066711324326 Acc: 0.7897371714643304\n",
      "Validation Loss: 0.5355939348588263 Acc: 0.8117206982543641\n",
      "Epoch 76\n",
      "Training Loss: 0.6027113792296495 Acc: 0.7903629536921152\n",
      "Validation Loss: 0.5443084243675718 Acc: 0.8154613466334164\n",
      "Epoch 77\n",
      "Training Loss: 0.615357627110726 Acc: 0.7913016270337923\n",
      "Validation Loss: 0.547651718679509 Acc: 0.8117206982543641\n",
      "Epoch 78\n",
      "Training Loss: 0.6083038125527517 Acc: 0.7909887359198998\n",
      "Validation Loss: 0.5343302570822233 Acc: 0.8229426433915212\n",
      "Epoch 79\n",
      "Training Loss: 0.6050312375843301 Acc: 0.7897371714643304\n",
      "Validation Loss: 0.5365453567216522 Acc: 0.8192019950124688\n",
      "Epoch 80\n",
      "Training Loss: 0.5916805922313686 Acc: 0.8003754693366708\n",
      "Validation Loss: 0.5301832032025307 Acc: 0.8204488778054863\n",
      "Epoch 81\n",
      "Training Loss: 0.6052568108775888 Acc: 0.7922403003754693\n",
      "Validation Loss: 0.5396565499151139 Acc: 0.8204488778054863\n",
      "Epoch 82\n",
      "Training Loss: 0.6037390355920612 Acc: 0.7922403003754693\n",
      "Validation Loss: 0.5413248611804553 Acc: 0.8129675810473815\n",
      "Epoch 83\n",
      "Training Loss: 0.6021843847628678 Acc: 0.7897371714643304\n",
      "Validation Loss: 0.5404493542680716 Acc: 0.814214463840399\n",
      "Epoch 84\n",
      "Training Loss: 0.5944895461593313 Acc: 0.7944305381727159\n",
      "Validation Loss: 0.5300747859953645 Acc: 0.8266832917705735\n",
      "Epoch 85\n",
      "Training Loss: 0.5909995285232315 Acc: 0.7981852315394243\n",
      "Validation Loss: 0.5350505530536918 Acc: 0.827930174563591\n",
      "Epoch 86\n",
      "Training Loss: 0.6069859860089604 Acc: 0.7878598247809763\n",
      "Validation Loss: 0.545820464851553 Acc: 0.8092269326683291\n",
      "Epoch 87\n",
      "Training Loss: 0.6000099561092105 Acc: 0.7900500625782227\n",
      "Validation Loss: 0.5241455793975297 Acc: 0.8291770573566085\n",
      "Epoch 88\n",
      "Training Loss: 0.590505937685507 Acc: 0.7916145181476846\n",
      "Validation Loss: 0.5646785392130996 Acc: 0.7980049875311721\n",
      "Epoch 89\n",
      "Training Loss: 0.6000942809411671 Acc: 0.7866082603254068\n",
      "Validation Loss: 0.5211673252600387 Acc: 0.8266832917705735\n",
      "Epoch 90\n",
      "Training Loss: 0.6022121864132052 Acc: 0.7812891113892365\n",
      "Validation Loss: 0.5372912714680531 Acc: 0.814214463840399\n",
      "Epoch 91\n",
      "Training Loss: 0.5952459053790315 Acc: 0.8016270337922403\n",
      "Validation Loss: 0.5273818047786889 Acc: 0.8229426433915212\n",
      "Epoch 92\n",
      "Training Loss: 0.5965498028246125 Acc: 0.7872340425531915\n",
      "Validation Loss: 0.5402380051532588 Acc: 0.8104738154613467\n",
      "Epoch 93\n",
      "Training Loss: 0.582403844005623 Acc: 0.7966207759699625\n",
      "Validation Loss: 0.5293462966148396 Acc: 0.814214463840399\n",
      "Epoch 94\n",
      "Training Loss: 0.6021929224381907 Acc: 0.7903629536921152\n",
      "Validation Loss: 0.5179672877390189 Acc: 0.830423940149626\n",
      "Epoch 95\n",
      "Training Loss: 0.593999804782032 Acc: 0.7909887359198998\n",
      "Validation Loss: 0.526366138380216 Acc: 0.8216957605985037\n",
      "Epoch 96\n",
      "Training Loss: 0.5785617609047919 Acc: 0.7900500625782227\n",
      "Validation Loss: 0.5405231925317474 Acc: 0.8154613466334164\n",
      "Epoch 97\n",
      "Training Loss: 0.5886702830710906 Acc: 0.7981852315394243\n",
      "Validation Loss: 0.5469836133376619 Acc: 0.8117206982543641\n",
      "Epoch 98\n",
      "Training Loss: 0.5748790865099624 Acc: 0.8028785982478097\n",
      "Validation Loss: 0.5238696934279063 Acc: 0.8254364089775561\n",
      "Epoch 99\n",
      "Training Loss: 0.5836519153604519 Acc: 0.7978723404255319\n",
      "Validation Loss: 0.5265049317531157 Acc: 0.8204488778054863\n",
      "Epoch 100\n",
      "Training Loss: 0.5787516460102401 Acc: 0.7991239048811014\n",
      "Validation Loss: 0.5278621463341977 Acc: 0.816708229426434\n",
      "Best val Acc: 0.830423940149626\n"
     ]
    }
   ],
   "source": [
    "net = train(net,loss,optimizer,num_epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the parameters of the desnet model after training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as trained_densenet.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(net.state_dict(), \"trained_densenet.pth\")  # Save model weights\n",
    "print(\"Model saved as trained_densenet.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
