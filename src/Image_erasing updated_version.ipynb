{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augemntation - Image Manipulation \n",
    "1. Geometric Transformations (Rotations, Translation, Shearing, Flipping)\n",
    "2. Non- Geometric Transformations (Cropping, Noise injection, Color Space, Jitter, Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import torchvision\n",
    "from torchvision import models,datasets,transforms\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import albumentations as A #this supposedly is much faster than using torchvision\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidia\\AppData\\Local\\Temp\\ipykernel_38316\\3229370424.py:37: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=7, max_height=40, max_width=40, fill_value=0, p=0.7),  # Increase erasure intensity\n",
      "C:\\Users\\vidia\\AppData\\Local\\Temp\\ipykernel_38316\\3229370424.py:40: UserWarning: Argument(s) 'value' are not valid for transform Erasing\n",
      "  A.Erasing(p=0.65, scale=(0.03, 0.4), ratio=(0.3, 3.5), value=0),  # Increased upper erasure limit\n"
     ]
    }
   ],
   "source": [
    "#version 2\n",
    "def hide_and_seek(image, **kwargs):  \n",
    "    \"\"\"\n",
    "    Applies a more controlled Hide-and-Seek augmentation to prevent excessive information loss.\n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        img = np.transpose(image, (2, 0, 1))  # Convert HWC -> CHW\n",
    "        img = img.copy()  \n",
    "    else:\n",
    "        img = image.clone() if isinstance(image, torch.Tensor) else image  \n",
    "\n",
    "    c, h, w = img.shape  \n",
    "    grid_sizes = [16, 32, 44]  # Removed large grid sizes\n",
    "    hide_prob = 0.3  # Lowered probability to avoid excessive hiding\n",
    "\n",
    "    grid_size = random.choice(grid_sizes)\n",
    "\n",
    "    for x in range(0, w, grid_size):\n",
    "        for y in range(0, h, grid_size):\n",
    "            x_end = min(w, x + grid_size)\n",
    "            y_end = min(h, y + grid_size)\n",
    "            if random.random() <= hide_prob:\n",
    "                img[:, y:y_end, x:x_end] = img.mean()  # Use mean value instead of black for soft erasing\n",
    "\n",
    "    if isinstance(image, np.ndarray):\n",
    "        img = np.transpose(img, (1, 2, 0))  \n",
    "\n",
    "    return img\n",
    "\n",
    "# New Augmentation Pipeline (Balanced Erasing)\n",
    "augmentation_pipeline = A.Compose([\n",
    "    A.CoarseDropout(max_holes=2, max_height=30, max_width=30, fill_value=0, p=0.4),  # Lowered impact\n",
    "    A.GridDropout(ratio=0.3, p=0.3),  # Reduced erasing coverage\n",
    "    A.Lambda(image=hide_and_seek, p=0.3),  # Hide-and-Seek with reduced probability\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),  # Introduce soft noise\n",
    "        A.Blur(blur_limit=3, p=0.3)  # Use blur instead of hard cutout\n",
    "    ], p=0.4),\n",
    "    # A.RandomErasing(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0),  # Corrected RandomErasing\n",
    "    ToTensorV2(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#varsion 3\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def hide_and_seek(image, **kwargs):  \n",
    "    \"\"\"\n",
    "    Enhanced Hide-and-Seek with better structured occlusions.\n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        img = np.transpose(image, (2, 0, 1))  # Convert HWC -> CHW\n",
    "        img = img.copy()  \n",
    "    else:\n",
    "        img = image.clone() if isinstance(image, torch.Tensor) else image  \n",
    "\n",
    "    c, h, w = img.shape  \n",
    "    grid_sizes = [6, 12, 24, 36]  # More fine-grained occlusions\n",
    "    hide_prob = 0.65  # Increase probability to erase more images\n",
    "\n",
    "    grid_size = random.choice(grid_sizes)\n",
    "\n",
    "    for x in range(0, w, grid_size):\n",
    "        for y in range(0, h, grid_size):\n",
    "            x_end = min(w, x + grid_size)\n",
    "            y_end = min(h, y + grid_size)\n",
    "            if random.random() <= hide_prob:\n",
    "                img[:, y:y_end, x:x_end] = img.mean()  # Smooth erasure\n",
    "\n",
    "    if isinstance(image, np.ndarray):\n",
    "        img = np.transpose(img, (1, 2, 0))  \n",
    "\n",
    "    return img\n",
    "\n",
    "# **FINAL Augmentation Pipeline for Maximum Erasing Impact**\n",
    "augmentation_pipeline = A.Compose([\n",
    "    A.CoarseDropout(max_holes=7, max_height=40, max_width=40, fill_value=0, p=0.7),  # Increase erasure intensity\n",
    "    A.GridDropout(ratio=0.35, p=0.7),  # More structured dropout\n",
    "    A.Lambda(image=hide_and_seek, p=0.65),  # Stronger Hide-and-Seek\n",
    "    A.Erasing(p=0.65, scale=(0.03, 0.4), ratio=(0.3, 3.5), value=0),  # Increased upper erasure limit\n",
    "    ToTensorV2(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cardboard': 368, 'glass': 336, 'metal': 632, 'paper': 400, 'plastic': 736, 'trash': 396}\n",
      "max category is 736 of class plastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing & Expanding Classes: 100%|██████████| 6/6 [1:31:37<00:00, 916.21s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset balanced & expanded! New images saved in ../data/dataset_erasing_02/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# use the pipeline to also balance the dataset\n",
    "# idea : augment images until limit set as majority_class*2\n",
    "\n",
    "\n",
    "# define the paths\n",
    "input_dir = \"../data/dataset_split/train\"  \n",
    "output_dir = \"../data/dataset_erasing_02/train\"  # for balanced augmented dataset\n",
    "\n",
    "# overwrite it\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)  # delete contents\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class_counts = {}\n",
    "for class_folder in os.listdir(input_dir):\n",
    "    class_path = os.path.join(input_dir, class_folder)\n",
    "    num_images = len(os.listdir(class_path))\n",
    "    class_counts[class_folder] = num_images\n",
    "\n",
    "# change this\n",
    "change_factor = 2\n",
    "\n",
    "# Find the largest class (majority class)\n",
    "max_class_size = max(class_counts.values())\n",
    "new_target_size = max_class_size * change_factor\n",
    "\n",
    "print(class_counts)\n",
    "print(f\"max category is {max_class_size} of class {max(class_counts, key=class_counts.get)}\") \n",
    "\n",
    "for class_folder, current_count in tqdm(class_counts.items(), desc=\"Balancing & Expanding Classes\"):\n",
    "    class_path = os.path.join(input_dir, class_folder)\n",
    "    augmented_class_path = os.path.join(output_dir, class_folder) #augmented images will be saved here\n",
    "    os.makedirs(augmented_class_path, exist_ok=True)\n",
    "\n",
    "    images = os.listdir(class_path)\n",
    "    \n",
    "    # decide later but this will copy all the non-augmented images as well \n",
    "    for img_name in images:\n",
    "        src_path = os.path.join(class_path, img_name)\n",
    "        dst_path = os.path.join(augmented_class_path, img_name)\n",
    "        cv2.imwrite(dst_path, cv2.imread(src_path))  # Copy image\n",
    "\n",
    "    # Compute number of extra images needed\n",
    "    num_needed = new_target_size - current_count  # balancing all classes to 2*majority\n",
    "\n",
    "    # Augment existing images\n",
    "    while num_needed > 0:\n",
    "        for img_name in images:\n",
    "            if num_needed <= 0:\n",
    "                break \n",
    "\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR (OpenCV) to RGB\n",
    "\n",
    "            augmented = augmentation_pipeline(image=image)[\"image\"]  # Albumentations output\n",
    "\n",
    "            # Save Augmented Image (Directly as PNG)\n",
    "            output_filename = f\"{os.path.splitext(img_name)[0]}_aug_{num_needed}.png\"\n",
    "            output_path = os.path.join(augmented_class_path, output_filename)\n",
    "\n",
    "            # Ensure the output is a NumPy array before saving\n",
    "            if isinstance(augmented, torch.Tensor):  \n",
    "                augmented = augmented.permute(1, 2, 0).cpu().numpy()  # Convert CHW -> HWC\n",
    "                augmented = (augmented * 255).astype(np.uint8)  # Convert from [0,1] to [0,255]\n",
    "\n",
    "            # Save the augmented image\n",
    "            cv2.imwrite(output_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
    "            num_needed -= 1\n",
    "\n",
    "print(f\"✅ Dataset balanced & expanded! New images saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardboard: 1472 images\n",
      "glass: 1472 images\n",
      "metal: 1472 images\n",
      "paper: 1472 images\n",
      "plastic: 1472 images\n",
      "trash: 1472 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to the main directory\n",
    "\n",
    "main_dir = \"../data/dataset_erasing_02/train\"\n",
    "# Dictionary to store image counts per subfolder\n",
    "image_counts = {}\n",
    "\n",
    "# Loop through each subdirectory\n",
    "for subfolder in os.listdir(main_dir):\n",
    "    subfolder_path = os.path.join(main_dir, subfolder)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Count images (considering common image formats)\n",
    "        image_count = len([file for file in os.listdir(subfolder_path) if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff'))])\n",
    "        \n",
    "        image_counts[subfolder] = image_count\n",
    "\n",
    "# Print the results\n",
    "for subfolder, count in image_counts.items():\n",
    "    print(f\"{subfolder}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sos remember to change the directory for the tensors\n",
    "dir = '../data/dataset_erasing/train'\n",
    "\n",
    "params = { 'batch_size':16,\n",
    "           'shuffle':True,\n",
    "           'num_workers':4 }\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.RandomResizedCrop(256), #Augmented\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "#the training dataset will be from the datase_balanced folder \n",
    "train_dataset = datasets.ImageFolder(os.path.join(dir, 'train'),transform = transform )\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(256),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "#change the dir for validationa (and test)\n",
    "dir = '../data/dataset_split'\n",
    "val_dataset = datasets.ImageFolder(os.path.join(dir, 'val'),transform = transform )\n",
    "\n",
    "\n",
    "#data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, **params)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, **params)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
