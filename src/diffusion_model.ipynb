{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Small dataset subset created at ../data/subset/train!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define paths\n",
    "dataset_root = \"../data/dataset_split/train\"  # Original dataset\n",
    "subset_root = \"../data/subset/train\"  # New small subset\n",
    "\n",
    "# Define how many images to copy per category\n",
    "num_images_per_class = 10  # Adjust as needed\n",
    "\n",
    "# Ensure the subset directory exists\n",
    "os.makedirs(subset_root, exist_ok=True)\n",
    "\n",
    "# Loop through each category and copy images\n",
    "for category in os.listdir(dataset_root):\n",
    "    class_path = os.path.join(dataset_root, category)\n",
    "    subset_class_path = os.path.join(subset_root, category)\n",
    "\n",
    "    os.makedirs(subset_class_path, exist_ok=True)\n",
    "\n",
    "    # Get a list of images in this category\n",
    "    images = os.listdir(class_path)\n",
    "    \n",
    "    # Select a random subset\n",
    "    selected_images = random.sample(images, min(num_images_per_class, len(images)))\n",
    "\n",
    "    # Copy selected images to new folder\n",
    "    for img_name in selected_images:\n",
    "        src_path = os.path.join(class_path, img_name)\n",
    "        dst_path = os.path.join(subset_class_path, img_name)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(f\"✅ Small dataset subset created at {subset_root}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 19 19:16:53 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 561.03                 Driver Version: 561.03         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   61C    P8              6W /  119W |     786MiB /   8188MiB |     15%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     28356    C+G   ...174_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A     35532    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     42752    C+G   ..._x64__cw5n1h2txyewy\\WidgetBoard.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# If you don't have a new enough diffusers, install/upgrade:\n",
    "# pip install --upgrade diffusers transformers accelerate safetensors\n",
    "\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "# ------------------------------------------\n",
    "# 0. Faster matmul on Ampere+ GPUs (optional)\n",
    "# ------------------------------------------\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1. Check if GPU supports float16\n",
    "# ------------------------------------------\n",
    "use_fp16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 7  # Ampere or newer\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. Load Stable Diffusion v1.5 in half-precision (Img2Img)\n",
    "# ------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if use_fp16:\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        revision=\"fp16\",   # ensures half-precision weights on Ampere+ (RTX 30 series, etc.)\n",
    "        torch_dtype=torch.float16,\n",
    "    ).to(device)\n",
    "else:\n",
    "    # Fallback to float32 if your GPU doesn't support half-precision\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float32,\n",
    "    ).to(device)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. Clear GPU cache (optional)\n",
    "# ------------------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Category-specific prompts\n",
    "# ------------------------------------------\n",
    "category_prompts = {\n",
    "    \"cardboard\": \"A high-resolution image of a discarded, crushed cardboard box with torn edges, slightly stained and placed on a rough concrete surface.\",\n",
    "    \"glass\":     \"A detailed photo of an empty, broken glass bottle lying on the ground with visible dirt and scratches.\",\n",
    "    \"metal\":     \"A high-quality image of a used aluminum can with dents and scratches, slightly rusty, placed in an outdoor recycling bin.\",\n",
    "    \"paper\":     \"A close-up of crumpled and torn waste paper, including newspapers and printed documents, slightly dirty and discarded.\",\n",
    "    \"plastic\":   \"A realistic image of a plastic milk bottle, partially crushed, with dirt marks on it, lying on a sidewalk.\",\n",
    "    \"trash\":     \"A pile of mixed waste including organic scraps, food leftovers, plastic wrappers, and used paper towels in a public trash bin.\"\n",
    "}\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. Define dataset paths\n",
    "# ------------------------------------------\n",
    "input_dir = \"../data/mlp_assignment/data/subset/train\"\n",
    "output_dir = \"../data/dataset_diffusion_balanced02/train\"\n",
    "\n",
    "# Overwrite existing output directory\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6. Count images per category\n",
    "# ------------------------------------------\n",
    "class_counts = {}\n",
    "for cat in os.listdir(input_dir):\n",
    "    cat_path = os.path.join(input_dir, cat)\n",
    "    if os.path.isdir(cat_path):\n",
    "        class_counts[cat] = len(os.listdir(cat_path))\n",
    "\n",
    "majority_class = max(class_counts, key=class_counts.get)\n",
    "majority_size = class_counts[majority_class]\n",
    "target_size = majority_size * 2  # Example: double the majority class\n",
    "\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "print(f\"Majority class: {majority_class} ({majority_size} images)\")\n",
    "print(f\"Target size per class: {target_size}\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 7. Augmentation parameters\n",
    "# ------------------------------------------\n",
    "strength = 0.5            # how much the new image differs from the original\n",
    "guidance_scale = 7.5      # typical range for SD1.5 is ~7-8\n",
    "image_size = (512, 512)   # desired output resolution\n",
    "num_images_per_prompt = 1 # how many outputs per single input at a time\n",
    "\n",
    "# ------------------------------------------\n",
    "# 8. Preprocessing function\n",
    "# ------------------------------------------\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize(image_size)\n",
    "    return image\n",
    "\n",
    "# ------------------------------------------\n",
    "# 9. Function to check if an image is too dark / black\n",
    "# ------------------------------------------\n",
    "def is_black_image(image, mean_threshold=10):\n",
    "    \"\"\"\n",
    "    Check if the image is predominantly black/dark by comparing\n",
    "    the average pixel intensity to a threshold.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = image.convert(\"L\")\n",
    "    arr = np.array(gray)\n",
    "\n",
    "    # If the mean is below a small threshold, consider it 'black'\n",
    "    return arr.mean() < mean_threshold\n",
    "\n",
    "# ------------------------------------------\n",
    "# 10. Balancing loop\n",
    "# ------------------------------------------\n",
    "for category, count in tqdm(class_counts.items(), desc=\"Balancing dataset\"):\n",
    "    class_path = os.path.join(input_dir, category)\n",
    "    augmented_class_path = os.path.join(output_dir, category)\n",
    "    os.makedirs(augmented_class_path, exist_ok=True)\n",
    "    \n",
    "    # 10a. Copy original images to new folder\n",
    "    images = os.listdir(class_path)\n",
    "    for img_name in images:\n",
    "        src_path = os.path.join(class_path, img_name)\n",
    "        dst_path = os.path.join(augmented_class_path, img_name)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    # 10b. Generate images until target size is reached\n",
    "    num_needed = target_size - count\n",
    "    \n",
    "    # Use the category-specific prompt if available; else fall back to a trivial prompt\n",
    "    prompt_text = category_prompts.get(category, \".\")\n",
    "\n",
    "    # We set an upper bound on the number of attempts to avoid infinite loops\n",
    "    max_attempts = num_needed * 4  # e.g., 4 attempts for each needed image\n",
    "    \n",
    "    attempts = 0\n",
    "    while num_needed > 0 and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        \n",
    "        # Randomly pick an existing image in the category\n",
    "        img_name = random.choice(images)\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        \n",
    "        # Preprocess input image (resize, convert to RGB)\n",
    "        input_image = preprocess_image(img_path)\n",
    "        \n",
    "        # SDEdit-style generation with Img2Img\n",
    "        with torch.no_grad():\n",
    "            result = pipe(\n",
    "                prompt=prompt_text,\n",
    "                image=input_image,\n",
    "                strength=strength,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_images_per_prompt=num_images_per_prompt,\n",
    "            )\n",
    "\n",
    "        synthetic_image = result.images[0]\n",
    "        \n",
    "        # Check if the image is black/dark\n",
    "        if is_black_image(synthetic_image):\n",
    "            # If it's black, skip saving and retry\n",
    "            # You could also print a warning:\n",
    "            # print(f\"Warning: Black image generated for {category}, retrying...\")\n",
    "            continue\n",
    "        \n",
    "        # If the image is okay, save it\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        output_filename = f\"{base_name}_aug_{num_needed}.png\"\n",
    "        output_path = os.path.join(augmented_class_path, output_filename)\n",
    "        synthetic_image.save(output_path)\n",
    "        \n",
    "        # One synthetic image successfully generated\n",
    "        num_needed -= 1\n",
    "\n",
    "print(f\"✅ Balanced dataset (SDEdit-style) saved at '{output_dir}'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Directory paths\n",
    "input_dir = \"../data/dataset_split/train\"\n",
    "output_dir = \"../data/dataset_diffusion_balanced_02/train\"\n",
    "\n",
    "# Overwrite existing output directory (if you want a clean slate)\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Img2Img parameters\n",
    "prompt = \"\"              # neutral or \"photo of [class]\"\n",
    "strength = 0.3\n",
    "guidance_scale = 4.0\n",
    "image_size = (512, 512)  # resize input to this before generation\n",
    "num_images_per_prompt = 1\n",
    "\n",
    "# Utility: Preprocess input (resize, etc.)\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize(image_size)\n",
    "    return image\n",
    "\n",
    "# Utility: Filter out very dark/black images\n",
    "def is_black_image(image, mean_threshold=10):\n",
    "    gray = image.convert(\"L\")\n",
    "    arr = np.array(gray)\n",
    "    return arr.mean() < mean_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3503d5a482f949589368c5e35d3277b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real image counts per category: {'cardboard': 368, 'glass': 336, 'metal': 632, 'paper': 400, 'plastic': 736, 'trash': 396}\n",
      "Target images per category: 1472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting categories:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting 'cardboard' with 1104 synthetic images.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7d0f2695c1470c8aa60c9638739dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting categories:   0%|          | 0/6 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 137\u001b[0m\n\u001b[0;32m    134\u001b[0m prompt, strength, guidance, num_inference_steps, eta \u001b[38;5;241m=\u001b[39m sample_diffusion_params(category)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 137\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m new_image \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_invalid_image(new_image):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion_img2img.py:1088\u001b[0m, in \u001b[0;36mStableDiffusionImg2ImgPipeline.__call__\u001b[1;34m(self, prompt, image, strength, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1085\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mscale_model_input(latent_model_input, t)\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1246\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[1;32m-> 1246\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1255\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block(sample, emb)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:920\u001b[0m, in \u001b[0;36mUNetMidBlock2DCrossAttn.forward\u001b[1;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m attn(\n\u001b[0;32m    913\u001b[0m             hidden_states,\n\u001b[0;32m    914\u001b[0m             encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    919\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 920\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\diffusers\\models\\resnet.py:346\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[1;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_time_act:\n\u001b[0;32m    345\u001b[0m         temb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlinearity(temb)\n\u001b[1;32m--> 346\u001b[0m     temb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_emb_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding_norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from diffusers import StableDiffusionImg2ImgPipeline, DDIMScheduler\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Set Up Diffusion Pipeline\n",
    "# ----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Faster on Ampere+ GPUs\n",
    "\n",
    "# Use half-precision (fp16) if supported\n",
    "use_fp16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 7\n",
    "\n",
    "if use_fp16:\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        revision=\"fp16\",\n",
    "        torch_dtype=torch.float16,\n",
    "    ).to(device)\n",
    "else:\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float32,\n",
    "    ).to(device)\n",
    "\n",
    "# Use DDIM scheduler for better control\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Define Directories\n",
    "# ----------------------------\n",
    "input_dir = \"../data/dataset_split/train\"  # Path to your real waste dataset\n",
    "output_dir = \"../data/diffusion_augmented_dataset/train\"  # Output directory for augmented images\n",
    "\n",
    "# Clear output directory if it exists\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Utility Functions\n",
    "# ----------------------------\n",
    "image_size = (512, 512)  # High resolution for better quality\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Load and resize image to model's expected resolution.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image.resize(image_size)\n",
    "\n",
    "def is_invalid_image(image, mean_threshold=10):\n",
    "    \"\"\"Skip nearly blank or dark images.\"\"\"\n",
    "    gray = image.convert(\"L\")\n",
    "    arr = np.array(gray)\n",
    "    return arr.mean() < mean_threshold\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Improved Prompt Setup for Feature Enhancement\n",
    "# ----------------------------\n",
    "# Define prompts that add category-specific features (e.g., dirt, scratches, textures)\n",
    "feature_prompts = {\n",
    "    \"cardboard\": \"A photo of a cardboard box with visible dirt and scratches, slightly crumpled.\",\n",
    "    \"glass\": \"A photo of a glass bottle with visible cracks and dirt, lying on a rough surface.\",\n",
    "    \"metal\": \"A photo of a metal can with dents and rust, placed in a recycling bin.\",\n",
    "    \"paper\": \"A photo of crumpled paper with visible stains and tears, partially folded.\",\n",
    "    \"plastic\": \"A photo of a plastic bottle with dirt marks and scratches, partially crushed.\",\n",
    "    \"trash\": \"A photo of mixed waste with visible food scraps, plastic wrappers, and dirt.\"\n",
    "}\n",
    "\n",
    "def sample_prompt(category):\n",
    "    \"\"\"Generate a feature-enhancing prompt for the given category.\"\"\"\n",
    "    return feature_prompts.get(category, \"A photo of waste.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Diffusion Parameter Sampling\n",
    "# ----------------------------\n",
    "base_strength = 0.3  # Low strength to preserve the original image\n",
    "base_guidance = 7.5  # Balanced guidance scale for better prompt adherence\n",
    "fixed_inference_steps = 40  # More steps for better quality\n",
    "fixed_eta = 0.6  # DDIM eta parameter\n",
    "\n",
    "def sample_diffusion_params(category):\n",
    "    strength = base_strength + random.uniform(-0.05, 0.05)  # ~0.25 to 0.35\n",
    "    guidance_scale = base_guidance + random.uniform(-0.5, 0.5)  # ~7.0 to 8.0\n",
    "    prompt = sample_prompt(category)\n",
    "    return prompt, strength, guidance_scale, fixed_inference_steps, fixed_eta\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Determine Target Count per Category\n",
    "# ----------------------------\n",
    "category_counts = {}\n",
    "for category in os.listdir(input_dir):\n",
    "    cat_path = os.path.join(input_dir, category)\n",
    "    if os.path.isdir(cat_path):\n",
    "        category_counts[category] = len(os.listdir(cat_path))\n",
    "print(\"Real image counts per category:\", category_counts)\n",
    "\n",
    "max_count = max(category_counts.values())\n",
    "target_count = max_count * 2  # Double the dataset size\n",
    "print(\"Target images per category:\", target_count)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Synthetic Data Generation Loop\n",
    "# ----------------------------\n",
    "for category, real_count in tqdm(category_counts.items(), desc=\"Augmenting categories\"):\n",
    "    cat_in = os.path.join(input_dir, category)\n",
    "    cat_out = os.path.join(output_dir, category)\n",
    "    os.makedirs(cat_out, exist_ok=True)\n",
    "    \n",
    "    # Copy original images into the augmented training set\n",
    "    for file in os.listdir(cat_in):\n",
    "        shutil.copy(os.path.join(cat_in, file), os.path.join(cat_out, file))\n",
    "    \n",
    "    synth_needed = target_count - real_count\n",
    "    if synth_needed <= 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"Augmenting '{category}' with {synth_needed} synthetic images.\")\n",
    "    real_files = os.listdir(cat_in)\n",
    "    attempts = 0\n",
    "    max_attempts = synth_needed * 4\n",
    "\n",
    "    while synth_needed > 0 and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        file_choice = random.choice(real_files)\n",
    "        img_path = os.path.join(cat_in, file_choice)\n",
    "        init_img = preprocess_image(img_path)\n",
    "        \n",
    "        prompt, strength, guidance, num_inference_steps, eta = sample_diffusion_params(category)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result = pipe(\n",
    "                prompt=prompt,\n",
    "                image=init_img,\n",
    "                strength=strength,\n",
    "                guidance_scale=guidance,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                eta=eta,\n",
    "                num_images_per_prompt=1,\n",
    "            )\n",
    "        new_image = result.images[0]\n",
    "        if is_invalid_image(new_image):\n",
    "            continue\n",
    "        \n",
    "        base, _ = os.path.splitext(file_choice)\n",
    "        out_filename = f\"{base}_synth_{synth_needed}.png\"\n",
    "        out_filepath = os.path.join(cat_out, out_filename)\n",
    "        print(f\"Saving generated image for '{category}': {os.path.abspath(out_filepath)}\")\n",
    "        new_image.save(out_filepath)\n",
    "        synth_needed -= 1\n",
    "\n",
    "print(\"✅ Diffusion-based augmentation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing dataset:   0%|          | 0/6 [00:00<?, ?it/s]Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a97180041b04147b6840f1395f7f414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c28844c4ab84578979ef26b79148044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f646293647745f8bacb57e30a2d8f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing dataset:  17%|█▋        | 1/6 [03:08<15:43, 188.75s/it]Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b536ac417033424b8228be1e7d921125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2436c6a2da46eb81cc53896c313dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abadedf398c24c308d38358a1fe7895c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing dataset:  33%|███▎      | 2/6 [06:50<13:51, 207.99s/it]Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9044ce46a72a40ab8f62134c91c48f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05267266532d48ee84841558c4fda4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818293e8897347b0a84f68144b9feb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing dataset:  50%|█████     | 3/6 [11:28<11:59, 239.97s/it]Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8de672b06f4a0d8a01ae43e73dd579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cde7587f054dc489f3c2cf064ae9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3f528ea40147a492705bb88c59c6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing dataset:  67%|██████▋   | 4/6 [16:13<08:35, 257.95s/it]Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355eeb502afc479980b38da39bd1bb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5869f8478fd14407b8ed5fcb590a2516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5a9d1f2aff49f489f8956ad4adeaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing dataset:  83%|████████▎ | 5/6 [20:25<04:15, 255.89s/it]Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d73b40cc26b414eb5f222f3451151c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (94 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or snack . the lighting is even , highlighting textures and reflections on the different materials .']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d8bf7f06194916a5dabbb5d6e6ba0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or snack . the lighting is even , highlighting textures and reflections on the different materials .']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab619b1d0e9f4bbdb0d084325798ffe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing dataset: 100%|██████████| 6/6 [26:11<00:00, 261.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Balanced dataset (Category-Specific Diffusion) saved at '../data/dataset_diffusion_balanced02/train'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define category-specific configurations\n",
    "category_settings = {\n",
    "  \"cardboard\": {\n",
    "    \"prompt\": \"A perfectly clean, brand-new cardboard box, neatly presented in {shapes}. It has sharp edges with no creases, dirt, or damage. The object is isolated on a seamless, bright white studio backdrop with soft, even lighting.\",\n",
    "    \"shapes\": [\"rectangular box\", \"square box\", \"flat folded box\", \"open-top box\", \"closed shipping box\", \"tall vertical box\"],\n",
    "    \"strength\": 0.8,\n",
    "    \"guidance_scale\": 8,\n",
    "    \"image_size\": [512, 512],\n",
    "    \"model\": \"runwayml/stable-diffusion-v1-5\"\n",
    "  },\n",
    "  \"glass\": {\n",
    "    \"prompt\": \"A high-resolution image of a transparent, unbroken {items} with crisp reflections and pristine glass. The object is isolated on a seamless, bright white studio backdrop with soft, even lighting.\",\n",
    "    \"items\": [\"glass bottle\", \"glass jar\", \"drinking glass\", \"wine glass\", \"glass tumbler\", \"glass vase\"],\n",
    "    \"strength\": 0.6,\n",
    "    \"guidance_scale\": 11,\n",
    "    \"image_size\": [512, 512],\n",
    "    \"model\": \"runwayml/stable-diffusion-v1-5\"\n",
    "  },\n",
    "  \"metal\": {\n",
    "    \"prompt\": \"A detailed close-up shot of a {items}, entirely free of dents, rust, or any damage. The object showcases vibrant colors and sharp details, isolated against a seamless, bright white studio backdrop with soft, even lighting.\",\n",
    "    \"items\": [\"aluminum can\", \"tin can\", \"food can\", \"soda can\", \"drink can\", \"beverage can\"],\n",
    "    \"strength\": 0.7,\n",
    "    \"guidance_scale\": 15,\n",
    "    \"image_size\": [512, 512],\n",
    "    \"model\": \"runwayml/stable-diffusion-v1-5\"\n",
    "  },\n",
    "  \"paper\": {\n",
    "    \"prompt\": \"A single, clean printer {items} lying flat on a seamless, bright white studio backdrop. The edges are perfectly straight, and the surface is pristine, free of any folds, dirt, or creases.\",\n",
    "    \"items\": [\"newspaper\", \"magazine\", \"printed document\", \"written paper sheet\", \"folded paper\", \"crumpled paper\", \"envelope\"],\n",
    "    \"strength\": 0.7,\n",
    "    \"guidance_scale\": 14,\n",
    "    \"image_size\": [512, 512],\n",
    "    \"model\": \"runwayml/stable-diffusion-v1-5\"\n",
    "  },\n",
    "  \"plastic\": {\n",
    "    \"prompt\": \"A single plastic {items}, perfectly shaped with a smooth, glossy surface, free of any scratches or dents. The object is placed on an isolated, seamless, bright white studio backdrop with soft, even lighting.\",\n",
    "    \"items\": [\"water bottle\", \"beverage bottle\", \"soda bottle\", \"juice bottle\", \"milk bottle\"],\n",
    "    \"strength\": 0.7,\n",
    "    \"guidance_scale\": 12,\n",
    "    \"image_size\": [512, 512],\n",
    "    \"model\": \"runwayml/stable-diffusion-v1-5\"\n",
    "  },\n",
    "  \"trash\": {\n",
    "    \"prompt\": \"A random assortment of discarded waste items on a plain surface, including crumpled paper, torn plastic wrappers, empty snack pouches, and used napkins. The materials vary between glossy plastic, metallic foil, and soft paper, with some items showing creases, stains, or slight tears. The objects are casually scattered, resembling everyday trash left behind after a meal or snack. The lighting is even, highlighting textures and reflections on the different materials.\",\n",
    "    \"strength\": 0.8,\n",
    "    \"guidance_scale\": 14,\n",
    "    \"image_size\": [512, 512],\n",
    "    \"model\": \"runwayml/stable-diffusion-v1-5\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "# Define dataset paths\n",
    "input_dir = \"../data/subset/train\"\n",
    "output_dir = \"../data/dataset_diffusion_balanced02/train\"\n",
    "\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Count images per category\n",
    "class_counts = {cat: len(os.listdir(os.path.join(input_dir, cat))) for cat in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, cat))}\n",
    "\n",
    "majority_class = max(class_counts, key=class_counts.get)\n",
    "majority_size = class_counts[majority_class]\n",
    "target_size = majority_size * 2  # Double the majority class\n",
    "\n",
    "def preprocess_image(image_path, size):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize(size)\n",
    "    return image\n",
    "\n",
    "def is_black_image(image, mean_threshold=10):\n",
    "    gray = image.convert(\"L\")\n",
    "    return np.array(gray).mean() < mean_threshold\n",
    "\n",
    "for category, count in tqdm(class_counts.items(), desc=\"Balancing dataset\"):\n",
    "    settings = category_settings.get(category, category_settings[\"trash\"])  # Default to \"trash\" settings if missing\n",
    "    \n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        settings[\"model\"],\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    ).to(device)\n",
    "    \n",
    "    class_path = os.path.join(input_dir, category)\n",
    "    augmented_class_path = os.path.join(output_dir, category)\n",
    "    os.makedirs(augmented_class_path, exist_ok=True)\n",
    "    \n",
    "    images = os.listdir(class_path)\n",
    "    for img_name in images:\n",
    "        shutil.copy(os.path.join(class_path, img_name), os.path.join(augmented_class_path, img_name))\n",
    "    \n",
    "    num_needed = target_size - count\n",
    "    print(num_needed)\n",
    "    max_attempts = num_needed * 4\n",
    "    attempts = 0\n",
    "    \n",
    "    while num_needed > 0 and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        img_name = random.choice(images)\n",
    "        input_image = preprocess_image(os.path.join(class_path, img_name), settings[\"image_size\"])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result = pipe(\n",
    "                prompt=settings[\"prompt\"],\n",
    "                image=input_image,\n",
    "                strength=settings[\"strength\"],\n",
    "                guidance_scale=settings[\"guidance_scale\"],\n",
    "                num_images_per_prompt=1,\n",
    "            )\n",
    "        \n",
    "        synthetic_image = result.images[0]\n",
    "        if is_black_image(synthetic_image):\n",
    "            continue\n",
    "        \n",
    "        output_filename = f\"{os.path.splitext(img_name)[0]}_aug_{num_needed}.png\"\n",
    "        synthetic_image.save(os.path.join(augmented_class_path, output_filename))\n",
    "        num_needed -= 1\n",
    "\n",
    "print(f\"✅ Balanced dataset (Category-Specific Diffusion) saved at '{output_dir}'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
