{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augemntation - Image Manipulation \n",
    "1. Geometric Transformations (Rotations, Translation, Shearing, Flipping)\n",
    "2. Non- Geometric Transformations (Cropping, Noise injection, Color Space, Jitter, Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidia\\AppData\\Roaming\\Python\\Python312\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.4' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import torchvision\n",
    "from torchvision import models,datasets,transforms\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import albumentations as A #this supposedly is much faster than using torchvision\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidia\\AppData\\Roaming\\Python\\Python312\\site-packages\\albumentations\\core\\validation.py:58: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\vidia\\AppData\\Local\\Temp\\ipykernel_11508\\1580372755.py:13: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(2, 10), p=0.2),  # less variance\n"
     ]
    }
   ],
   "source": [
    "#create augmentation pipeline\n",
    "#all of these need to be tuned for now these are some safe choices\n",
    "\n",
    "augmentation_pipeline = A.Compose([\n",
    "    # Geometric Transformations\n",
    "    A.Rotate(limit=25, p=0.5),  # rotation +-25 with prob 0.5\n",
    "    A.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.2, rotate_limit=0, p=0.3),  # translation (shift and scale but do not rotate more)\n",
    "    A.Affine(shear=5, p=0.5),  # shearing up to 5deg - does not seem that useful for us\n",
    "    A.HorizontalFlip(p=0.5),  # flipping h\n",
    "\n",
    "    # Non-Geometric Transformations\n",
    "    A.RandomResizedCrop(size=[524,524], scale=(0.7, 1.0), p=0.5),  # gets randomly 70-100% of image and resized it back to 256x256\n",
    "    A.GaussNoise(var_limit=(2, 10), p=0.2),  # less variance\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.2), # contrast for color (insetad of b&w)\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.3),  # Color Adjustments different lightning conditions\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.2),  # Kernel Blurring\n",
    "\n",
    "    A.Resize(524, 524),  # resizing to 524x524 (then using tensor pipeline will be 256x256)\n",
    "\n",
    "    ToTensorV2() #this computes it back to tensor - to use for pytorch\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cardboard': 368, 'glass': 336, 'metal': 632, 'paper': 400, 'plastic': 736, 'trash': 396}\n",
      "max category is 736 of class plastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing & Expanding Classes: 100%|██████████| 6/6 [01:49<00:00, 18.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset balanced & expanded! New images saved in ../data/dataset_balanced/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# use the pipeline to also balance the dataset\n",
    "# idea : augment images until limit set as majority_class*2\n",
    "\n",
    "# if you want to test for yourself then update directories to \"../example_data/dataset_balanced/train\"\n",
    "output_dir = \"../data/dataset_balanced/train\"\n",
    "\n",
    "\n",
    "# define the paths\n",
    "input_dir = \"../data/dataset_split/train\"  \n",
    "output_dir = \"../data/dataset_balanced/train\"  # for balanced augmented dataset\n",
    "\n",
    "# overwrite it\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)  # delete contents\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class_counts = {}\n",
    "for class_folder in os.listdir(input_dir):\n",
    "    class_path = os.path.join(input_dir, class_folder)\n",
    "    num_images = len(os.listdir(class_path))\n",
    "    class_counts[class_folder] = num_images\n",
    "\n",
    "# change this\n",
    "change_factor = 2\n",
    "\n",
    "# Find the largest class (majority class)\n",
    "max_class_size = max(class_counts.values())\n",
    "new_target_size = max_class_size * change_factor\n",
    "\n",
    "print(class_counts)\n",
    "print(f\"max category is {max_class_size} of class {max(class_counts, key=class_counts.get)}\") \n",
    "\n",
    "for class_folder, current_count in tqdm(class_counts.items(), desc=\"Balancing & Expanding Classes\"):\n",
    "    class_path = os.path.join(input_dir, class_folder)\n",
    "    augmented_class_path = os.path.join(output_dir, class_folder) #augmented images will be saved here\n",
    "    os.makedirs(augmented_class_path, exist_ok=True)\n",
    "\n",
    "    images = os.listdir(class_path)\n",
    "    \n",
    "    # decide later but this will copy all the non-augmented images as well \n",
    "    for img_name in images:\n",
    "        src_path = os.path.join(class_path, img_name)\n",
    "        dst_path = os.path.join(augmented_class_path, img_name)\n",
    "        cv2.imwrite(dst_path, cv2.imread(src_path))  # Copy image\n",
    "\n",
    "    # Compute number of extra images needed\n",
    "    num_needed = new_target_size - current_count  # balancing all classes to 2*majority\n",
    "\n",
    "    # Augment existing images\n",
    "    while num_needed > 0:\n",
    "        for img_name in images:\n",
    "            if num_needed <= 0:\n",
    "                break \n",
    "\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR (OpenCV) to RGB\n",
    "\n",
    "            augmented = augmentation_pipeline(image=image)[\"image\"]  # Albumentations output\n",
    "\n",
    "            # Save Augmented Image (Directly as PNG)\n",
    "            output_filename = f\"{os.path.splitext(img_name)[0]}_aug_{num_needed}.png\"\n",
    "            output_path = os.path.join(augmented_class_path, output_filename)\n",
    "\n",
    "            # Ensure the output is a NumPy array before saving\n",
    "            if isinstance(augmented, torch.Tensor):  \n",
    "                augmented = augmented.permute(1, 2, 0).cpu().numpy()  # Convert CHW -> HWC\n",
    "                augmented = (augmented * 255).astype(np.uint8)  # Convert from [0,1] to [0,255]\n",
    "\n",
    "            # Save the augmented image\n",
    "            cv2.imwrite(output_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
    "            num_needed -= 1\n",
    "\n",
    "print(f\"✅ Dataset balanced & expanded! New images saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sos remember to change the directory for the tensors\n",
    "dir = '../data/dataset_balanced'\n",
    "\n",
    "params = { 'batch_size':16,\n",
    "           'shuffle':True,\n",
    "           'num_workers':4 }\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.RandomResizedCrop(256), #Augmented\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "#the training dataset will be from the datase_balanced folder \n",
    "train_dataset = datasets.ImageFolder(os.path.join(dir, 'train'),transform = transform )\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(256),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "#change the dir for validationa (and test)\n",
    "dir = '../data/dataset_split'\n",
    "val_dataset = datasets.ImageFolder(os.path.join(dir, 'val'),transform = transform )\n",
    "\n",
    "\n",
    "#data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, **params)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, **params)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
